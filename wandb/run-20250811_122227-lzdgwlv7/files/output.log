Training Configuration:
----------------------------------------
substrate_name      : coins
num_players         : 7
model_type          : attention_influence
feature_dim         : None
action_dim          : None
hidden_dim          : 256
num_episodes        : 5000
lr                  : 0.0003
gamma               : 0.99
eps_clip            : 0.2
k_epochs            : 4
influence_weight    : 0.1
curriculum_steps    : 1000000
device              : cuda
save_interval       : 500
log_interval        : 10
use_wandb           : True
wandb_project       : item-aware-social-influence
wandb_entity        : None
----------------------------------------
================================================================================
Starting training: attention_influence on coins
================================================================================
Creating environment: coins
[DEBUG] Substrate 'coins' info:
  - Mandated players: 2
  - Default players: 2
  - Valid roles: ['default']
[WARNING] Substrate 'coins' requires exactly 2 players.
[WARNING] Ignoring requested 7 players, using 2.
[DEBUG] Final configuration:
  - Using 2 players
  - Using roles: ('default', 'default')
Structured feature dimension: 20
  - Reset state features: 20
  - After step features: 20
Number of players: 2
[DEBUG] Using max feature dimension: 20

=== OBSERVATION DEBUG ===
Observation type: <class 'dict'>
Observation keys: ['COLLECTIVE_REWARD', 'MISMATCHED_COIN_COLLECTED_BY_PARTNER', 'RGB', 'WORLD.RGB']
  COLLECTIVE_REWARD: shape (), dtype float64
  MISMATCHED_COIN_COLLECTED_BY_PARTNER: shape (), dtype float64
  RGB: shape (88, 88, 3), dtype uint8
  WORLD.RGB: shape (136, 136, 3), dtype uint8
=========================


=== ENVIRONMENT VALIDATION ===
Reset observation type: <class 'dict'>
Number of agents: 2
First agent observation type: <class 'dict'>
✓ Structured features extracted: 20 dimensions
✓ Item positions extracted: (0, 2)
==============================

[INFO] Updating trainer to use 2 players instead of requested 7
Creating attention_influence trainer...
Feature dim: 20, Action dim: 7, Players: 2
[DEBUG] AttentionActorCritic init:
  - feature_dim: 20
  - attention_output_dim: 128
  - total_input_dim: 148
[DEBUG] AttentionActorCritic init:
  - feature_dim: 20
  - attention_output_dim: 128
  - total_input_dim: 148
Starting training for 5000 episodes...
[DEBUG] Raw features shape: 28
[DEBUG] Expected feature_dim: 20
[DEBUG] Features: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]...
[WARNING] Feature dimension mismatch! Got 28, expected 20
[WARNING] Truncated features to 20
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 8.64267349243164e-06
  - attention_weight: 1.0
  - weighted_influence: 8.64267349243164e-06
  - total_influence: 8.64267349243164e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([0, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([0, 2])
  - relative_items shape: torch.Size([0, 2])
  - NO NEIGHBORS: returning empty tensors
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] Agent 0 feature tensor shape: torch.Size([20])
[DEBUG] Neighbor positions shape: torch.Size([1, 2])
[DEBUG] Item positions shape: torch.Size([0, 2])
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
  - keys shape: torch.Size([1, 64])
  - attn_input shape: torch.Size([1, 192])
  - attn_logits shape: torch.Size([1])
  - attn_weights shape: torch.Size([1])
  - attn_weights values: tensor([1.], device='cuda:0')
  - context shape: torch.Size([64])
  - FINAL: returning attn_weights shape torch.Size([1])
  - context device: cuda:0
  - mean_item_emb device: cuda:0
  - combined_features shape: torch.Size([148]), device: cuda:0
  - expected total_input_dim: 148
[DEBUG] select_action for agent 0:
  - neighbor_positions shape: torch.Size([1, 2])
  - item_positions shape: torch.Size([0, 2])
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
[DEBUG] compute_attention_influence_reward for agent 0:
  - neighbor_indices: [1]
  - attention_weights: tensor([1.], device='cuda:0')
  - attention_weights type: <class 'torch.Tensor'>
  - attention_weights shape: torch.Size([1])
  - attention_weights numel: 1
  - Processing neighbor 1 at idx 0
  - attention_weights[0] = 1.0
  - kl_divergence: 7.361173629760742e-06
  - attention_weight: 1.0
  - weighted_influence: 7.361173629760742e-06
  - total_influence: 7.361173629760742e-06
[DEBUG] Forward pass:
  - my_features shape: torch.Size([20]), device: cuda:0
  - neighbor_positions shape: torch.Size([1, 2]), device: cuda:0
  - item_positions shape: torch.Size([0, 2]), device: cuda:0
  - expected feature_dim: 20
[DEBUG] AttentionEmbeddingModel.forward:
  - relative_neighbors shape: torch.Size([1, 2])
  - relative_items shape: torch.Size([0, 2])
  - neighbor_embs shape: torch.Size([1, 64])
  - NO ITEMS: using zero item embedding
Training interrupted by user
